import pandas as pd
import aisuite as ai
from tqdm import tqdm
from dotenv import load_dotenv, find_dotenv
from def_analysis import get_function_definitions
from contextlib import contextmanager
from utils import sanity_check, run_test, extract_code
from ratelimit import sleep_and_retry, limits

from openai import OpenAI

load_dotenv(find_dotenv())
client = ai.Client()
client.configure({
  "together" : {
    "timeout": 1000,
  },
  "openai": {
      "timeout": 1000,
  },
  "google":{
      "timeout": 1000
  }})

# client = OpenAI(api_key="sk-88387e1f90604e5d938f0ab797e7e722", base_url="https://api.deepseek.com")

models = [
    "together:meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
    "together:deepseek-ai/DeepSeek-R1",
    "openai:o3-mini"
]

# r1 on together tier 1 has 3 reqs/minute
@sleep_and_retry
@limits(calls=3, period=60)
def call_deepseek(message):
    response = client.chat.completions.create(
                model="together:deepseek-ai/DeepSeek-R1", 
                messages=[{"role": "user", "content": message.strip()}])
    generated_code = response.choices[0].message.content.strip()
    return generated_code

def check(llm_output, expected_answer):
    """
    Verify if the model's answer is mathematically equivalent to the expected answer
    using GPT-o3-mini as a judge.
    
    Args:
        llm_output (str): The answer generated by the model
        expected_answer (str): The expected correct answer
    
    Returns:
        str: "YES" if the answers are consistent, "NO" otherwise
    """
    verification_prompt = f"""
    As a mathematical equivalence checker, determine if these two answers are mathematically equivalent:
    
    MODEL ANSWER: {llm_output}
    EXPECTED ANSWER: {expected_answer}
    
    Consider all forms of mathematical equivalence. For example:
    - Different representations like 0.5, 1/2, and 50% are equivalent
    - Different LaTeX notations for the same mathematical entity are equivalent
    - Simplified vs expanded forms of the same expression are equivalent
    
    Respond with ONLY 'YES' if they are equivalent or 'NO' if they are not.
    """
    
    response = client.chat.completions.create(
        model="together:meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo", 
        messages=[{"role": "user", "content": verification_prompt.strip()}]
    )
    
    result = response.choices[0].message.content.strip()
    
    # Return YES if the response contains YES, otherwise return NO
    if "YES" in result.upper():
        return "YES"
    else:
        return "NO"

file_path = "Marvel Math Packaged Data.xlsx"  # Replace with the path to your .xlsx file
data = pd.read_excel(file_path)

# Initialize dictionary to track results for each model
model_results = {}

# Run benchmark for each model
for model in models:
    print(f"\n=== Running benchmark for model: {model} ===\n")
    
    # Reset counters for this model
    total = 0
    exact_matches = 0
    
    # Iterate over each row, query the LLM, and evaluate accuracy
    for index, row in data.iterrows():
        prompt = f'''You are a Math Problem Solver. Your task is to solve the given mathematical problem and provide the final answer in a specific format. Follow these instructions carefully:
            1. Read and understand the mathematical problem presented to you.
            2. Solve the problem step by step in your mind, ensuring accuracy in your calculations and reasoning.
            3. Once you have the final answer, format it as follows:
            a. Present the answer in a single line.
            b. Use LaTeX format to properly display mathematical notation.
            c. Do not include any explanations, workings, or additional text.
            Here is the mathematical problem you need to solve:
            <problem>
    '''

        prompt += row["User_Prompt"]
        prompt+="</problem>"
        prompt+="Remember to provide the final answer in a single word as minimal as possible in LaTeX format.If your answer is true or false return Yes or No"
        expected_answer = (row["Answer"]) # Ensure uniform formatting
        
        # Special handling for DeepSeek-R1 due to rate limiting
        if "deepseek-ai/DeepSeek-R1" in model:
            llm_output = call_deepseek(prompt)
        else:
            # Generate the response from the LLM
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "system",
                        "content": "You are a Math Problem Solver",
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
            )
            llm_output = response.choices[0].message.content
        
        print(f"Model: {model}")
        print(f"Output: {llm_output}")
        print(f"Expected: {expected_answer}")
        
        flag = check(llm_output, expected_answer)
        print(f"Match: {flag}")
        
        if "YES" in flag.upper():
            exact_matches += 1
       
        total += 1
        print(f"Processed row {index + 1}/{len(data)} for model {model}")
    
    # Calculate and store accuracy for this model
    accuracy = exact_matches / total * 100
    model_results[model] = {
        "total_problems": total,
        "correct_answers": exact_matches,
        "accuracy": accuracy
    }
    print(f"Model: {model} - Accuracy: {accuracy:.2f}%")

# Display summary of all model results
print("\n=== Benchmark Results ===")
for model, results in model_results.items():
    print(f"Model: {model}")
    print(f"  Problems: {results['total_problems']}")
    print(f"  Correct: {results['correct_answers']}")
    print(f"  Accuracy: {results['accuracy']:.2f}%")
    print()